## Interpretability with DeepDream Assignment

The purpose of this assignment is to familiarize students with the techniques discussed in the presentation for Interpretability with DeepDream.

#### Part 1

For part one, click on the link to each Colab Notebook and read through the implementation. after reviewing the implementation, run the code snippets to familiarize yourself with what the code is doing.

1. [Channel Attribution](https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/building-blocks/AttrChannel.ipynb)
2. [Semantic Dictionaries](https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/building-blocks/SemanticDictionary.ipynb)
3. [Activation Grids](https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/building-blocks/ActivationGrid.ipynb)
4. [Spatial Attribution](https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/building-blocks/AttrSpatial.ipynb)
5. [Neuron Groups](https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/building-blocks/NeuronGroups.ipynb)

#### Part 2

For part two, answer the following questions:
1. Which interface did you find most useful?
2. What do you find most interesting about interpretability?
3. Can you think of an interface not presented? How about a was to improve an existing one?
4. Explain each of the above techniques in 5 words or less.